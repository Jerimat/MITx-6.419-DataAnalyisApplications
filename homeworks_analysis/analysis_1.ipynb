{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import t\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compton Gamma Ray Observatory Data set\n",
    "### Goal:\n",
    "Check the assumption that the emission rate is constant\n",
    "\n",
    "#### Data\n",
    "- X: Sequential time interval *(of variable lengths)*\n",
    "- Y: Number of gamma rays originating in a particular area in the sky\n",
    "\n",
    "#### Model\n",
    "The phenomenon of interest seems to be related to a Poisson process. Therefore, we will model the data as\n",
    "\n",
    "$G_i \\sim Poisson(\\lambda_i t_i)$\n",
    "where\n",
    "\n",
    "- $\\lambda_i$ : average rate of gamma rays in interval i\n",
    "- $t_i$ : length in second of the time interval\n",
    "\n",
    "#### Hypothesis\n",
    "$H_0: \\lambda_0 = \\lambda_1 = ... = \\lambda_{99}$\n",
    "\n",
    "$H_A: \\lambda_i \\neq \\lambda_j$ for some *i and j*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('./data/gamma-ray.csv')\n",
    "data = df.copy()\n",
    "t_i = data.loc[:, 'seconds']\n",
    "G_i = data.loc[:, 'count']\n",
    "\n",
    "data.columns = ['t_i', 'G_i']\n",
    "data['lambda_i'] = G_i / t_i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.scatter(t_i, G_i)\n",
    "plt.xlabel('Sequential time interval (seconds)')\n",
    "plt.ylabel('Number of gamma rays (count)')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compute the Maximum Likelihood Estimators (MLE) $\\hat{\\lambda}$ for the parameter $\\lambda$:\n",
    "\n",
    "| MLE | $H_0$ is true | $H_A$ is true |\n",
    "| --- | ------------- | ------------- |\n",
    "| $\\hat{\\lambda}$ | $\\frac{ \\sum_i{ \\frac{G_i}{t_i}}} {\\sum_i{t_i} }$ | $\\frac{G_i}{t_i}$ |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MLE_0 = np.sum(data.G_i) / np.sum(data.t_i)\n",
    "MLE_A = data.G_i / data.t_i\n",
    "\n",
    "print('MLE given H_0 is true: %.3g' % MLE_0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(t_i, poisson.pmf(G_i, MLE_0), 'bo', ms=3)\n",
    "plt.title('Poisson distribution with parameters $\\lambda$ = %.3g' % MLE_0)\n",
    "plt.xlabel('Sequential time interval (seconds)')\n",
    "plt.ylabel('Number of gamma rays (count)')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Likelihood Ratio Test\n",
    "L = -2 * np.log((np.product(poisson.pmf(data.G_i, MLE_0))) / (np.product(poisson.pmf(data.G_i, MLE_A))))\n",
    "print('Likelihood Ratio Test L = {:.3f}'.format(L))\n",
    "d = MLE_A.shape[0] - 1\n",
    "\n",
    "significance_level = 0.05\n",
    "R_region = chi2.ppf(1 - significance_level, d)\n",
    "print('Rejection region: L(x) >= {:.3f}'.format(R_region))\n",
    "\n",
    "p_value = chi2.sf(L, d)\n",
    "print('p-value for L(x): {:.3f}'.format(p_value))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the chi^2 distribution and p-value\n",
    "x = np.arange(chi2.ppf(0.001, d), chi2.ppf(0.999, d))\n",
    "plt.plot(x, chi2.pdf(x, d))\n",
    "plt.vlines(L, 0, chi2.pdf(L, d), colors='g', label='L={:.3f}'.format(L))\n",
    "plt.fill_between(x, chi2.pdf(x, d), where=x>=R_region, color='red', label='Rejection region (level={})'.format(significance_level))\n",
    "plt.title('Chi^2 distribution with {} degrees of freedom\\np-value = {:.3f}'.format(d, p_value))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the p-value is greater than our significance level (0.337 > 0.05), we fail to reject $H_0$.\n",
    "This means that the data does not significantly (at a significance level 0.05) show that $\\lambda_i \\neq \\lambda_j$ for some i, j and the emission rate appears to be constant."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Molecular classification of cancer (Golub et al. (1999)) dataset\n",
    "### Goal:\n",
    "Discover how many genes can be used to differentiate tumor types (acute lymphoblastic leukemia (ALL) and acute myeloid leukemia (AML)).\n",
    "To do that we will use:\n",
    "- uncorrected p-values\n",
    "- Holm-Bonferoni correction, and Benjamini-Hochberg correction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import data set\n",
    "## Expression levels for the 38 tumor mRNA samples (row=genes, columns=mRNA samples)\n",
    "gene_expression = pd.read_csv('./data/golub_data/golub.csv')\n",
    "tumor_class = pd.read_csv('./data/golub_data/golub_cl.csv') # 0 -> ALL, 1 -> AML\n",
    "\n",
    "gene_expression.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "tumor_class.drop('Unnamed: 0', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gene_expression.head()\n",
    "# tumor_class.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We separate the dataset into two data sets:\n",
    "# - One with the expression levels for genes across the ALL mRNA samples\n",
    "# - One with the expression levels for genes across the AML mRNA samples\n",
    "indicator = tumor_class['x'].values.astype(bool)\n",
    "AML_genes = gene_expression.iloc[:,indicator]\n",
    "ALL_genes = gene_expression.iloc[:, indicator==False]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "barX_ALL = np.asarray(np.mean(ALL_genes, axis=1))\n",
    "barX_AML = np.asarray(np.mean(AML_genes, axis=1))\n",
    "\n",
    "N_ALL = ALL_genes.shape[1]\n",
    "N_AML = AML_genes.shape[1]\n",
    "\n",
    "s2_ALL = np.var(np.asarray(ALL_genes), axis=1, ddof=1)\n",
    "s2_AML = np.var(np.asarray(AML_genes), axis=1, ddof=1)\n",
    "\n",
    "s2_barX_ALL = s2_ALL / N_ALL\n",
    "s2_barX_AML = s2_AML / N_AML"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Welch unequal variances t-test\n",
    "The Welch unequal variances t-test is similar to the t-test.\n",
    "\n",
    "$t_{Welch}=\\frac{|\\bar{X}_{ALL} - \\bar{X}_{AML}|}{\\sqrt{\\frac{s^2_{ALL}}{N_{ALL}}+{\\frac{s^2_{AML}}{N_{AML}}}}} \\sim t_\\nu$.\n",
    "\n",
    "As we consider a two-sided test, we could either use the absolute value of the difference in means.\n",
    "\n",
    "The degrees of freedom parameter is given by the Welch-Satterthwaite formula:\n",
    "$\\nu = \\frac{(\\frac{s^2_{ALL}}{N_{ALL}}+{\\frac{s^2_{AML}}{N_{AML}}})^2}\n",
    "{\\frac{1}{\\nu_{ALL}}(\\frac{s^2_{ALL}}{N_{ALL}})^2 + \\frac{1}{\\nu_{AML}}(\\frac{s^2_{AML}}{N_{AML}})^2}$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# delta_barX = barX_ALL - barX_AML\n",
    "delta_barX = np.abs(barX_ALL - barX_AML)\n",
    "s_delta_barX = s2_barX_ALL + s2_barX_AML\n",
    "\n",
    "t_Welch = delta_barX / np.sqrt(s_delta_barX)\n",
    "\n",
    "nu_ALL = N_ALL - 1\n",
    "nu_AML = N_AML - 1\n",
    "nu = np.floor((s_delta_barX ** 2) / (((1 / nu_ALL) * s2_barX_ALL ** 2) + ((1 / nu_AML) * s2_barX_AML ** 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "significance_level = 0.05  / 2  # We divide the significance level by two as we consider a two-sided test\n",
    "R_region = t.ppf(1 - significance_level, nu)\n",
    "p_values = t.sf(t_Welch, nu)\n",
    "\n",
    "# If absolute value is NOT used for delta_barX\n",
    "# significant_p_values = np.concatenate((p_value[np.where(p_value <= significance_level)[0]],\n",
    "#                                       p_value[np.where(p_value >= 1 - significance_level)]))\n",
    "#If absolute value is used for delta_barX\n",
    "significant_p_values = (p_values[np.where(p_values <= significance_level)[0]])\n",
    "significant_t_Welch = t_Welch[np.where(np.abs(t_Welch) >= R_region)[0]]\n",
    "\n",
    "print('Number of significant genes: (uncorrected p-values)', len(significant_p_values))\n",
    "print('Number of |t_Welch| <= (Rejection region)_i: {}\\n'.format(len(significant_t_Welch)))\n",
    "# print('significant p-values:\\n', np.round(significant_p_values, 4))\n",
    "# print('\\nsignificant t_Welch:\\n', np.round(significant_t_Welch, 4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# As the different gene expression have different means and variances, we cannot plot all the results in one graph.\n",
    "i = 10\n",
    "nu_significant = nu[np.where(np.abs(t_Welch) >= R_region)[0]]\n",
    "R_significant = R_region[np.where(np.abs(t_Welch) >= R_region)[0]]\n",
    "\n",
    "x = np.linspace(t.ppf(0.0001, nu_significant[i]), t.ppf(0.9999, nu_significant[i]), 100)\n",
    "plt.plot(x, t.pdf(x, nu_significant[i]))\n",
    "plt.plot(significant_t_Welch[i], t.pdf(significant_t_Welch[i], nu_significant[i]), 'ro', markersize=3)\n",
    "plt.fill_between(x, t.pdf(x, nu_significant[i]), where=np.abs(x)>=R_significant[i], alpha=0.3, color='r', lw=1)\n",
    "\n",
    "plt.title('p-value: {:.3f}'.format(significant_t_Welch[i]))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "significance_level = 0.05\n",
    "# Welch t-test using scipy.stats.ttest_ind (equal_var must be False so that Welch t-test is performed)\n",
    "t_Welch, p_values = ttest_ind(ALL_genes, AML_genes, axis=1, equal_var=False)\n",
    "significant_p_values = np.concatenate((p_values[np.where(p_values <= significance_level)], p_values[np.where(p_values >= 1 - significance_level)]))\n",
    "print('Number of significant p-values using ttest_ind:', len(significant_p_values))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We performed multiple hypothesis testing but we did not correct for the p-values used to determine if we should reject $H_0$ or fail to reject it.\n",
    "\n",
    "We will now perform such corrections, first by using the Holm-Bonferroni correction and then the Benjamini-Hochberg correction.\n",
    "\n",
    "_Holm-Bonferroni correction:_ Reject ${H_0}_i$ if $p_i \\leq \\frac{\\alpha}{m}$ where m is the number of hypotheses performed.\n",
    "\n",
    "_Benjamini-Hochberg correction:_\n",
    "   1. Sort the p-values in increasing order\n",
    "   2. Find the maximum k such that $p_k \\leq \\frac{k * \\alpha}{m}$\n",
    "   3. Reject all the ${H_0}_i$ where $i \\leq k$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Using libraries\n",
    "# Number of significantly associated genes using p-value corrections\n",
    "## Holm-Bonferroni correction\n",
    "HM_correction = multipletests(p_values, alpha=significance_level, method='holm')\n",
    "significant_HM_p_values = t_Welch[HM_correction[0]]\n",
    "print('Significant p-values using the Holm-Bonferroni correction: ', len(significant_HM_p_values))\n",
    "\n",
    "## Benjamini-Hochberg correction\n",
    "BH_correction = multipletests(p_values, alpha=significance_level, method='fdr_bh')\n",
    "significant_BH_p_values = t_Welch[BH_correction[0]]\n",
    "print('Significant p-values using the Benjamini-Hochberg correction: ', len(significant_BH_p_values))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Implementation\n",
    "def holm_bonferroni_correction(uncorrected_p_values, significance_level):\n",
    "    sorted_p_values = np.sort(uncorrected_p_values)\n",
    "\n",
    "    m = sorted_p_values.shape[0]\n",
    "    i = 0\n",
    "    while sorted_p_values[i] <= significance_level / (m - i + 1):\n",
    "        i += 1\n",
    "\n",
    "    significant_HM_p_values = sorted_p_values[:i]\n",
    "\n",
    "    return significant_HM_p_values\n",
    "\n",
    "def benjamini_hochberg_correction(uncorrected_p_values, significance_level):\n",
    "    sorted_p_values = np.sort(uncorrected_p_values)\n",
    "\n",
    "    m = sorted_p_values.shape[0]\n",
    "    i = m-1\n",
    "    while sorted_p_values[i] > (i * significance_level) / m and i >= 0:\n",
    "        i -= 1\n",
    "\n",
    "    significant_BH_p_values = sorted_p_values[:i]\n",
    "    return significant_BH_p_values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "significant_HM_p_values = holm_bonferroni_correction(p_values, significance_level)\n",
    "print('Significant p-values using the Holm-Bonferroni correction: ', len(significant_HM_p_values))\n",
    "# print(significant_HM_p_values)\n",
    "\n",
    "significant_BH_p_values = benjamini_hochberg_correction(p_values, significance_level)\n",
    "print('Significant p-values using the Benjamini-Hochberg correction: ', len(significant_BH_p_values))\n",
    "# print(significant_BH_p_values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ordinary Least Squares (OLS) and Gradient Descent Algorithms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    return np.concatenate((np.ones_like(X[:, :1]), X), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import data\n",
    "X = np.genfromtxt('./data/syn_X.csv', delimiter=',', dtype=float)\n",
    "y = np.genfromtxt('./data/syn_y.csv', delimiter=',', dtype=float)\n",
    "\n",
    "X = add_intercept(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.shape\n",
    "# y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OLS_estimate = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), y)\n",
    "print('beta_hat (OLS):', np.round(OLS_estimate, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_convergence(losses, parameters, nb_iterations, step_size):\n",
    "    iterations = np.arange(nb_iterations + 1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, sharex=True, figsize=(10, 5))\n",
    "    title = 'final loss={:.3g}'\n",
    "    ax[0].plot(iterations, losses, color='blue')\n",
    "    ax[0].set_title(title.format(losses[-1]))\n",
    "\n",
    "    for i in range(parameters.shape[1]):\n",
    "        beta_i = parameters[:, i]\n",
    "        ax[1].plot(iterations, beta_i, label='$\\\\beta_{}$'.format(i))\n",
    "\n",
    "    ax[1].set_title('Convergence of parameters')\n",
    "\n",
    "    fig.suptitle('Gradient Descent, Loss: Squared Loss\\nStep size: {}\\nConverged after {} iterations'.format(step_size, nb_iterations))\n",
    "    plt.legend(frameon=True)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss(beta, X, y):\n",
    "    '''\n",
    "    Squared error (loss) function to minimize\n",
    "    '''\n",
    "    return np.sum((np.matmul(beta, X.T) - y) ** 2)\n",
    "\n",
    "def gradient_loss(beta, X, y):\n",
    "    '''\n",
    "    Gradient of the squared error function to minimize\n",
    "    '''\n",
    "    return 2 * np.matmul(np.matmul(beta, X.T) - y, X)\n",
    "\n",
    "def gradient_descent(X, y, step_size, precision):\n",
    "    '''\n",
    "    Perform the Gradient Descent algorithm with 'step_size' (eta).\n",
    "    Convergence is considered reached when the change in loss is less than 'precision'\n",
    "    :param X: (ndarray) Feature matrix\n",
    "    :param y: (ndarray) Dependent variable\n",
    "    :param step_size: (int) step size by which to update the parameters with the gradient of the loss\n",
    "    :param precision: (float)\n",
    "    :return: OLS parameters, number of iteration until convergence\n",
    "    '''\n",
    "    n_sample, n_features = X.shape\n",
    "    w_t = np.zeros(n_features)\n",
    "\n",
    "    losses = []\n",
    "    parameters = []\n",
    "\n",
    "    gradient_step = lambda w_t: w_t - step_size * gradient_loss(w_t, X, y)\n",
    "    change_in_loss = lambda w_t, w_t_1: np.abs(loss(w_t, X, y) - loss(w_t_1, X, y))  ## difference between the loss at w_t and the loss at w_t+1\n",
    "    # change_in_loss = lambda w_t: np.linalg.norm(gradient_f(w_t, X, y), 2)  ## squared norm of the gradient of the loss at w_t+1\n",
    "\n",
    "    w_t_1 = gradient_step(w_t)\n",
    "\n",
    "    losses.append(loss(w_t_1, X, y))\n",
    "    parameters.append(w_t_1)\n",
    "\n",
    "    nb_iterations = 0\n",
    "    while change_in_loss(w_t, w_t_1) >= precision:\n",
    "        w_t = w_t_1\n",
    "        w_t_1 = gradient_step(w_t)\n",
    "\n",
    "        nb_iterations += 1\n",
    "        losses.append(loss(w_t_1, X, y))\n",
    "        parameters.append(w_t_1)\n",
    "\n",
    "    visualize_convergence(losses, np.asarray(parameters), nb_iterations, step_size)\n",
    "\n",
    "    return (w_t_1, nb_iterations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_gradient_descent(X, y, step_size=None):\n",
    "    precision = 10e-6\n",
    "\n",
    "    if step_size is not None:\n",
    "        OLS_parameters, nb_iterations = gradient_descent(X, y, step_size=step_size, precision=precision)\n",
    "    else:\n",
    "        step_size = np.arange(0.001, 0.01, 0.001)\n",
    "\n",
    "        nb_iterations = []\n",
    "        OLS_parameters = []\n",
    "        for alpha in step_size:\n",
    "            OLS_GD, nb_iter = gradient_descent(X, y, step_size=alpha, precision=precision)\n",
    "            nb_iterations.append(nb_iter)\n",
    "            OLS_parameters.append(OLS_GD)\n",
    "\n",
    "    return OLS_parameters, nb_iterations, step_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_best_run(OLS_parameters, nb_iterations, step_size):\n",
    "    min_iterations = np.argmin(nb_iterations)\n",
    "    best_alpha = step_size[min_iterations]\n",
    "    parameters_best_alpha = OLS_parameters[min_iterations]\n",
    "\n",
    "    return best_alpha, parameters_best_alpha"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OLS_parameters, nb_iterations, step_size = run_gradient_descent(X, y)\n",
    "\n",
    "best_alpha, parameters_best_alpha = get_best_run(OLS_parameters, nb_iterations, step_size)\n",
    "print('best alpha:', best_alpha)\n",
    "print('OLS: ', np.round(parameters_best_alpha, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### General Motors data set - Study of the contribution of air pollution to mortality"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('./data/mortality.csv')\n",
    "y = df.Mortality\n",
    "X = df.loc[:, df.columns != y.name].select_dtypes(['float64', 'int64'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "X.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_array = add_intercept(np.asarray(X))\n",
    "OLS_parameters, nb_iterations, step_size = run_gradient_descent(X_array, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems that no matter the step_size used in our Gradient Descent algorithm, the algorithm will not converge.\n",
    "\n",
    "**Why would that be ?**\n",
    "\n",
    "We will try to find out by first visualizing the distribution of our parameters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normalize = True\n",
    "\n",
    "def plot_distributions(normalize=False, log_transform=False):\n",
    "    columns = X.columns\n",
    "\n",
    "    fig, ax = plt.subplots(len(columns), 1, figsize=(5, 30))\n",
    "    fig.subplots_adjust(hspace=1)\n",
    "\n",
    "    for i, column in enumerate(columns):\n",
    "        x_i = X.loc[:, column]\n",
    "        barX = np.mean(x_i)\n",
    "        s_x = np.var(x_i)\n",
    "\n",
    "        if normalize is True:\n",
    "            x_i = (x_i - barX) / np.sqrt(s_x)\n",
    "\n",
    "        if log_transform is True:\n",
    "            x_i = np.log(x_i)\n",
    "\n",
    "        ax[i].hist(x_i)\n",
    "        ax[i].set_title('X={}\\nE[X]={:.3f}, var(X)={:.3f}'.format(column, barX, s_x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_distributions(normalize=False, log_transform=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_distributions(normalize=False, log_transform=False)\n",
    "# plt.hist(y, label='Mortality')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now try again the GD algorithm with the normalized data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_array = np.asarray(X)\n",
    "X_normalized = (X_array - np.mean(X_array, axis=0)) / np.sqrt(np.var(X_array, axis=0))\n",
    "y_normalized = (y - np.mean(y)) / np.sqrt(np.var(y))\n",
    "X_array = add_intercept(np.asarray(X_normalized))\n",
    "\n",
    "OLS_parameters, nb_iterations, step_size = run_gradient_descent(X_array, y_normalized)\n",
    "\n",
    "best_alpha, parameters_best_alpha = get_best_run(OLS_parameters, nb_iterations, step_size)\n",
    "print('best alpha:', best_alpha)\n",
    "print('OLS: ', np.round(parameters_best_alpha, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_array = add_intercept(X_normalized)\n",
    "OLS_parameters, nb_iterations, step_size = run_gradient_descent(X_array, y_normalized, 0.004)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print('OLS parameters: ', np.round(OLS_parameters, 3))\n",
    "# for param in OLS_parameters:\n",
    "#     print('%.3f' % param)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OLS_estimate = np.matmul(np.matmul(np.linalg.inv(np.matmul(X_array.T, X_array)), X_array.T), y_normalized)\n",
    "print('beta_hat (OLS):', np.round(OLS_estimate, 3))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}